# Configuration for SFT Data Processing
# This configuration processes existing datasets into TrainingText format for supervised fine-tuning

# @package _global_
defaults:
  - _self_
  - streams: local
  - wandb: default

# Experiment configuration
output_dir: ???
seed: 42
force_restart: false

# Model configuration
model_path: /mnt/llmd/base_models/Qwen2.5-7B
max_seq_length: 2048
batch_size: 100

# Dataset configuration
dataset_loader: pipelinerl.domains.math.load_datasets
dataset_loader_params: {}
train_dataset_names:
  - open_reasoner_zero_57k
  - open_reasoner_zero_extended_72k
test_dataset_names:
  - aime_2024
  - amc_2023
  - math_500

# Subset configuration (optional)
train_subset: null

# LLM configuration for tokenization
me:
  llm_urls: "http://localhost:8000"  # Single URL for tokenization

llm:
  parameters:
    temperature: 1.0
    top_p: 0.95
    top_k: 50

# Debug configuration
debug:
  mode: ""
  streams_from: null

# Wandb configuration
wandb:
  use_wandb: true
  project: "sft-data-processing"
  name: null
  tags: ["sft", "data-processing"]

# Streams configuration
streams:
  backend: local
  base_path: null
