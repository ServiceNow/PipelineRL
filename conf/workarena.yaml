defaults:
  - base
  - override streams: redis
  - override finetune: grpo
  - _self_

world:
  actor_fraction: 2
  preprocessor_fraction: 0
  finetune_fraction: 4

save_tapes: true

output_dir: results/workarena/${now:%Y-%m-%d}/${now:%H-%M-%S}
model_path: meta-llama/Llama-3.1-8B-Instruct
use_ray: true

finetune:
  seq_length: 16384  # input + output tokens
  max_train_steps: 1000  # 1000 optim steps = 1000 * bs samples
  train_batch_size: 1
  gradient_accumulation_passes: 512
  rl:
    filter_zero_advantage_groups: true
    divide_advantage_by_std: true

eval_every_n_versions: 1024  # 1024 effective bs * 10 "optim steps"

llm:
  use_cache: false
  parameters:
    max_tokens: 4096  # output tokens
    temperature: 1.0
test_llm:
  parameters:
    max_tokens: ${...llm.parameters.max_tokens}
    temperature: 0.0
    top_p: 1.0
    top_k: 50

vllm_config:
  use_v1: false
  vllm_kwargs:
    max-num-seqs: 256
    max-num-batched-tokens: 32000
    max_model_len: 16384
    gpu-memory-utilization: 0.9

actor:
  rollout_policy: pipelinerl.domains.workarena.rollouts.generate_workarena_rollout
  llm_max_rollouts: 256
  problem_queue_size: 2
  async_batch_size: 1
  rollout_workers: 32
  shared_memory_entry_size: 100000000
  collect_logprobs: false
  discount_factor: 0.98  # discount factor for reward computation

preprocess:
  n_workers: 32  # Increase from 8
  chunk_n_groups: 8  # Increase from 2 for better throughput
  # queue for loaded raw groups
  raw_queue_size: 32      # Increase from 8
  # queue for processed chunks of multiple groups
  input_queue_size: 64    # Increase from 32
  # queue for ready chunks for multiple groups
  output_queue_size: 64   # Increase from 32
  # ring buffer to replace old samples with new ones when training is slow
  ring_buffer_size: 1024  # Increase from 128
  # "virtual" sample queue per lead trainer
  max_ready_samples_per_lead: 256  # Increase from 64
  shared_memory_entry_size: 1000000000  # Increase from 100M

# AGENT CONFIGURATION
agent_max_loops: 30  # max number of agent - environment interactions for each task
agent_attempts: 3  # number of attempts to run the agent (retry on errors)
rollout_timeout: 600  # overall timeout for entire rollout in seconds (10 minutes)
reward_computation: massimo # discounted reward based on the length of the trajectory and nico reward is discounted on number of retries at a step

use_generic_agent: true  # use GenericWebAgent vs TapeAgents Agent
generic_agent:
  max_iterations: 30
  use_examples: true
  max_chars_page_observation: 10000
  max_retries: 3
  include_think_in_history: false
agent:
  _target_: examples.workarena.agent.WorkArenaAgent

# ENVIRONMENT CONFIGURATION
start_attempts: 3  # number of attempts to start each task
environment:
  _target_: pipelinerl.domains.workarena.environment.WorkArenaEnvironment
  exp_path: ${output_dir}/browser
  headless: true
  observation_format: axtree

# DATASET CONFIGURATION
dataset_loader: pipelinerl.domains.workarena.load_tasks.load_tasks
# Available splits: train/test (main), debug_train/debug_test
dataset_loader_params:
  train_seeds: null  # seeds for train splits (default: [10-999])
  test_seeds: null   # seeds for test splits (default: [0-999])
  total_eval_episodes: 100  # number of eval episodes (cycles through seeds)
train_dataset_names:
  - train
test_dataset_names:
  - test